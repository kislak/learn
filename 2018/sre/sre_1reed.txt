- Introduction

tenets of sre

SRE team is responsible for the
  availability,
  latency,
  performance,
  efficiency,
  change management,
  monitoring,
  emergency response,
  capacity planning

conduct a postmortem
blame-free postmortem culture

Pursuing Maximum Change Velocity Without Violating a Service’s SLO

There are three kinds of valid monitoring output:
alert
ticket
logging

Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR)
  R = mttf/mttr

machine - rack - row - cluster - datacenter - campus
borg - distributed cluster operating system
bns - borg name service (name ->  ip:port)
hdfs - Hadoop Distributed File System
Google File System -> Colossus (BigTable, Spanner, Blobstore)
Global Software Load Balancer (GSLB)
  dns
  service
  RPC
// name, {BNS list -> capacity (RPS)}

The Chubby lock service provides - Paxos protocol for asynchronous Consensus

monitoring:
Set up alerting for acute problems.
Compare behavior: did a software update make the server faster?
Examine how resource consumption behavior evolves over time, which is essential for capacity planning.
Stubby - gRPC

- Principles
- 3 Embracing risk (принять(охватывать, видеть) риски)

99% - availability
availability = uptime/update+downtime
availability = 200/404

types of error effects (sign up vs 1 feature)

cost vs profit
  if we were to build +9 in availability
    what would incremental increase in revenue be?

falling between 0.01% and 1%

error budget

Hope is not a strategy

- 4 Service Level Objectives
SLI
  what user want (from intuition)
  quantitative measure of some aspect of the level of service

  request latency
  error rate
  system throughput
  availability
  durability (storage)
  correctness

- 5 Eliminating Toil
  operational work = toil
  Manual
  Repetitive
  Automatable
  Tactical
  No enduring value
  O(n) with service growth

  invent more, toil less.

- 6 Monitoring Distributed Systems
  monitoring
    Collecting, processing, aggregating, and displaying real-time quantitative data about a system
  White-box monitoring
  Black-box monitoring
  Dashboard
  Alert
  Root cause
  Node and machine
  Push
    any change


  monitor
    analyze long-term trends
    comparing(time/group)
    alerting
    building dashboards
    Conducting ad hoc retrospective analysis (i.e., debugging)

The four golden signals of monitoring are
 latency,
 traffic,
 errors,
 saturation (memory, i/o, cpu)

To remedy the situation, the team used a three-pronged approach

- 7 The Evolution of Automation at Google
value
  consistency
  platform (infrastructure)
  faster repair (mean time to repare MTTR)
  faster action
  time saving

meta-software - software to act on software

flaky tests
Service-Oriented Architecture (SOA)
  service owners would be responsible for creating an Admin Server to handle cluster turnup/turndown RPCs
  each team would provide the contract (API) that the turnup automation needed

- 8 Release Engineering
  building and delivering software

tools
  source code management
  compilers
  build configuration languages
  automated build tools
  package managers
  installers

skills
  development
  configuration management
  test integration
  system administration
  customer support

philosophy
  self-service model
  high velocity
  hermetic builds
  enforcement of policies and procedures (pipelines)

define release processes
define release process policies

- 9 Simplicity
The price of reliability is the pursuit of the utmost simplicity.

stability vs agility
minimizing accidental complexity
  essential complexity and accidental complexity

minimal api (encapsulation)
modularity
small release

Part III. Practices
maslow:

            product         (27: reliable product launches at scale)
          development       (23: critical state, 24: cron, 25: pipeline, 26: data integrity)
        capacity planning   (18: se in sre, 19: LB frontend, 20: LB in datacenter, 21: handling overload, 22: cascading failure)
      testing release       (17: testing for reliability)
    root cause analysis     (15: learn from failure, 16: tracking outage)
  incident response         (11: on-call, 12: troubleshoot, 13: emergency, 14: incidents)
monitoring                  (10: time-series data)

- 10 Practical Alerting from Time-Series Data
monitoring with Prometheus
  https://www.safaribooksonline.com/videos/kubernetes-and-prometheus/9781492037378/9781492037378-video319658
  the beginning of a beautiful friendship
  four golden signals
    https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/
    latency
    traffic
    errors
    saturation
  USE (Brendan Gregg)
    utilization
    saturation
    errors
  RED (Tom Wilkie)
    request
      rate
      error
      duration

white-box - (unit tests, integration tests) monitoring system—it inspects the internal state of the target service
black-box - (feature tests) - contract checks (higher abstraction - horizon)

// label - tag - distribution list

- 11 Being On-Call
working and nonworking hours
responsible for
  performance
  reliability
5, 30 min - mobile phone
2 peaople (primary and secondary)
6 peaople team

6 hours per incident:
  root-cause analysis,
  remediation,
  follow-up activities
    writing a postmortem
    fixing bugs

2 per 12-hour on-call shift.

50 - engineering
25 - on-call
25 - other type operational

postmortem

ways of thinking
  Intuitive, automatic, and rapid action
  Rational, focused, and deliberate cognitive functions

stress hormones
  cortisol
  corticotropin-releasing hormone (CRH)
  are known to cause behavioral consequences—including fear
  that can impair cognitive functions and cause suboptimal decision making
  While intuition and quick reactions can seem like desirable traits in the middle of incident management, they have downsides.
  Intuition can be wrong and is often less supportable by obvious data.
  Thus, following intuition can lead an engineer to waste time pursuing a line of reasoning that is incorrect from the start.

the most important on-call resources:
  clear escalation paths
  well-defined incident-management procedures
  a blameless postmortem culture

upper bound for the incident’s time span

# postmortems after significant incidents and detail a full timeline of the events that occurred

"give back the pager"
"Wheel of Misfortune"

- 12 Effective Troubleshooting

problem report
{
  triage
  examine
  diagnose
  test and treat
}
cure

if a bug is leading to possibly unrecoverable data corruption,
freezing the system to prevent further failure may be better than letting this behavior continue.
This realization is often quite unsettling and counterintuitive

use well-documented negative result
publish your results

postmortem
  tracked down the problem,
  how you fixed the problem,
  and how to prevent it from happening again.

Making Troubleshooting Easier
  building observability
  design systems with well-understood and observable interfaces between components

- 13 Emergency Response
If you feel overwhelmed(потрясен, поражен, ошеломлен), pull in more people.
Ask the Big, Even Improbable, Questions: What If…?
  Could you minimize the impact if it were to happen now?
  Could the person sitting next to you do the same?
Encourage Proactive Testing

- 14 Managing Incidents
Recursive Separation of Responsibilities

roles
  Incident Command (war room - irc)
  Operational Work
  Communication
  Planning

Incident State Document
  Summary:
  Status:
  Command:
    post(s): tel, email, chat
    Hierarchy (all responders)
      Current Incident Commander: jennifer
      Operations lead: docbrown
      Planning lead: jennifer
      Communications lead: jennifer
      Next Incident Commander: to be determined
  Detailed Status (last updated at)
    Exit Criteria:
    TODO list and bugs filed:
    Incident timeline:

BEST PRACTICES
Prioritize.
  Stop the bleeding, restore service, and preserve the evidence for root-causing.
Prepare.
  Develop and document your incident management procedures in advance, in consultation with incident participants.
Trust.
  Give full autonomy within the assigned role to all incident participants.
Introspect.
  Pay attention to your emotional state while responding to an incident.
  If you start to feel panicky or overwhelmed, solicit more support.
Consider alternatives.
  Periodically consider your options and re-evaluate whether it still makes sense to continue
  what you’re doing or whether you should be taking another tack in incident response.
Practice.
  Use the process routinely so it becomes second nature.
Change it around.
  Were you incident commander last time?
  Take on a different role this time.
  Encourage every team member to acquire familiarity with each role.

- 15 Postmortem Culture: Learning from Failure
Shakespeare Sonnet++ Postmortem (incident #465)
  Date: 2015-10-21
  Authors: jennifer, martym, agoogler
  Status: Complete, action items in progress
  Summary:
  Impact: estimated 1.21B queries lost, no revenue impact.
  Root Causes:
  Trigger: Latent bug triggered by sudden increase in traffic.
  Resolution:
  Detection: Borgmon detected high level of HTTP 500s and paged on-call.
  Action Items:
  Lessons Learned:
    WHAT WENT WELL
    WHAT WENT WRONG
    WHERE WE GOT LUCKY
  Timeline:

Postmortem Culture
  Postmortem of the month
  Google+ postmortem group
  Wheel of Misfortune

BEST PRACTICE:
  NO POSTMORTEM LEFT UNREVIEWED
  VISIBLY REWARD PEOPLE FOR DOING THE RIGHT THING
  ASK FOR FEEDBACK ON POSTMORTEM EFFECTIVENESS

- 16 Tracking Outages (простой, бездействие)
tagging alerts
aggregate alerts
(stats: component per week/month/)

- 17 Testing for Reliability
SDLC (waterfall)
  requirements
  design
  dev
  qa
  maintenance

key responsibility of SRE
  quantify confidence in the systems

Prognoses about future
  The site remains completely unchanged
  or
  You can confidently describe all changes

If you make too many changes too quickly,
the predicted reliability approaches the acceptability limit.
At this point, you may want to stop making changes while new monitoring data accumulates.


Zero MTTR occurs when a system-level test is applied to a subsystem
  Cucumber + Сapybara

tests
  unit
  integration
  system
    smoke
    performance
    regression
production tests (black-box testing)
configuration tests
stress tests
  - database (how ful)
  - how many queries
canary test
  the incubation period for the upgraded servers - “baking the binary.”

When the build system notifies engineers about broken code,
they should drop all of their other tasks and prioritize fixing the problem.

some areas of test coverage need a higher level of paranoia than others

The role of SRE generally includes writing systems engineering tools

- 18 Software Engineering in SRE
team size should not scale directly with service growth
Bin packing is an NP-hard problem that is difficult for human beings to compute by hand

Intent-Based Capacity Planning
  specify the requirements, not the implementation.
  specify intent over implementation

The basic premise
  encode the dependencies and parameters

precursors to intent
  dependencies
  performance metrics
  prioritization

SREs are used to working closely with their teammates, quickly analyzing and reacting to problems.

- 19 Load Balancing at the Frontend
